{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "plt.style.use(\"bmh\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "The model is loosely based on [Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting\n",
    "](https://arxiv.org/abs/1907.00235). We do not implement \"learnable\" positional encoding, allowing convolutions to do the job. Outputs are distribution parameters, similar to DeepAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/AEP_hourly.csv\", parse_dates=[\"Datetime\"], index_col=\"Datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fix timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.index.is_monotonic, df.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = df.sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### New index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "new_idx = pd.date_range(\"2004-10-01 01:00:00\", \"2018-08-03 00:00:00\", freq=\"1H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = df[~df.index.duplicated(keep='first')].reindex(new_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.index.is_monotonic, dfi.index.is_unique, dfi.index.freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Various model blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# PyTorch Lightning imports\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1D(pl.LightningModule):\n",
    "    \"\"\"Causal convolution for transformer model.\"\"\"\n",
    "   \n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 padding_mode: str = 'zeros'):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding_mode = padding_mode\n",
    "        self.padding_requirement = kernel_size - 1\n",
    "\n",
    "        self.layer = nn.Conv1d(self.in_channels,\n",
    "                               self.out_channels,\n",
    "                               self.kernel_size,\n",
    "                               padding=self.padding_requirement,\n",
    "                               padding_mode=self.padding_mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_out = self.layer(x)\n",
    "        return x_out[:, :, :-self.padding_requirement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.Tensor(np.random.randn(1, 4, 100))\n",
    "\n",
    "causal_conv = CausalConv1D(4, 8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = causal_conv(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_conv.layer.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 max_len: int = 168,\n",
    "                 dropout: float = 0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(dim, max_len)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim)).unsqueeze(1)\n",
    "        pe[0::2, :] = torch.sin(position * div_term)\n",
    "        pe[1::2, :] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :, :x.size(-1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc = PositionalEncoding(8, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc.pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pos_enc.pe.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc(test_output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pos_enc(test_output).detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_output.detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `TransformerEncoder` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenc_layer = nn.TransformerEncoderLayer(8, 8, 16)\n",
    "test_enc = tenc_layer(pos_enc(test_output).permute(2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenc_layer.self_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ElectricityDataset(Dataset):\n",
    "    \"\"\"Dataset which samples the data from hourly electricity data.\"\"\"\n",
    "\n",
    "    def __init__(self, df, samples, hist_len=168, fct_len=24, col=\"AEP_MW\"):\n",
    "        self.hist_num = hist_len\n",
    "        self.fct_num = fct_len\n",
    "        self.hist_len = pd.Timedelta(hours=hist_len)\n",
    "        self.fct_len = pd.Timedelta(hours=fct_len)\n",
    "        self.offset = pd.Timedelta(hours=1)\n",
    "\n",
    "        self.max_ts = df.index.max() - self.hist_len - self.fct_len + self.offset\n",
    "        self.raw_data = df\n",
    "\n",
    "        assert samples <= self.raw_data[:self.max_ts].shape[0]\n",
    "        self.samples = samples\n",
    "        self.col = col\n",
    "        self.sample()\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Sample individual series as needed.\"\"\"\n",
    "        \n",
    "        self.sample_idx = (self\n",
    "                           .raw_data[:self.max_ts]\n",
    "                           .index\n",
    "                           .to_series()\n",
    "                           .sample(self.samples, replace=False)\n",
    "                           .index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_ts = self.sample_idx[idx]\n",
    "\n",
    "        hs, he = start_ts, start_ts + self.hist_len - self.offset\n",
    "        fs, fe = he + self.offset, he + self.fct_len\n",
    "\n",
    "        hist_data = self.raw_data[hs:].iloc[:self.hist_num]\n",
    "        fct_data = self.raw_data[fs:].iloc[:self.fct_num]\n",
    "\n",
    "        return (torch.Tensor(hist_data[self.col].values).unsqueeze(0),\n",
    "                torch.Tensor(fct_data[self.col].values).unsqueeze(0))\n",
    "\n",
    "\n",
    "class ElectricityDataModule(pl.LightningDataModule):\n",
    "    \"\"\"DataModule for electricity data.\"\"\"\n",
    "\n",
    "    def __init__(self, df,\n",
    "                 train_range=(\"2004\", \"2015\"),\n",
    "                 val_range=(\"2016\",\"2017\"),\n",
    "                 test_range=(\"2018\", None),\n",
    "                 factor=0.5,\n",
    "                 batch_size=64,\n",
    "                 workers=3):\n",
    "\n",
    "        super().__init__()\n",
    "        self.raw_data = df\n",
    "        self.train_range = train_range\n",
    "        self.val_range = val_range\n",
    "        self.test_range = test_range\n",
    "        self.factor = factor\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            train_df = self.raw_data[slice(*self.train_range)]\n",
    "            val_df = self.raw_data[slice(*self.val_range)]\n",
    "\n",
    "            self.train_ds = ElectricityDataset(train_df,\n",
    "                                               samples=int(self.factor * train_df.shape[0]))\n",
    "            self.val_ds = ElectricityDataset(val_df,\n",
    "                                             samples=int(self.factor * val_df.shape[0]))\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            test_df = self.raw_data[slice(*self.test_range)]\n",
    "            self.test_ds = ElectricityDataset(test_df,\n",
    "                                              samples=int(self.factor * test_df.shape[0]))\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, num_workers=self.workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, num_workers=self.workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.batch_size, num_workers=self.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ElectricityDataset(dfi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectricityLoadTransformer(pl.LightningModule):\n",
    "    \"\"\"ransformer model for electricity load forecasting.\"\"\"\n",
    "   \n",
    "    def __init__(self,\n",
    "                 hist_len: int = 168,\n",
    "                 fct_len: int = 24,\n",
    "                 kernels: int = 8,\n",
    "                 kernel_size: int = 5,\n",
    "                 pos_dropout: float = 0.1,\n",
    "                 tr_dropout: float = 0.1,\n",
    "                 heads: int = 8,\n",
    "                 dim_feedforward: int = 32,\n",
    "                 lr: float = 1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hist_len = hist_len\n",
    "        self.fct_len = fct_len\n",
    "        self.kernels = kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pos_dropout = pos_dropout\n",
    "        self.tr_dropout = tr_dropout\n",
    "        self.heads = heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.conv_encoder = CausalConv1D(1, self.kernels, self.kernel_size)\n",
    "        self.pos_encoder = PositionalEncoding(self.kernels, self.hist_len+self.fct_len, self.pos_dropout)\n",
    "        self.transformer_block = nn.TransformerEncoderLayer(d_model=self.kernels,\n",
    "                                                            nhead=self.heads,\n",
    "                                                            dim_feedforward=self.dim_feedforward,\n",
    "                                                            dropout=self.tr_dropout)\n",
    "\n",
    "        self.mu = nn.Linear(in_features=self.kernels, out_features=1)\n",
    "        self.sigma_raw = nn.Linear(in_features=self.kernels, out_features=1)\n",
    "        self.sigma = nn.Softplus()\n",
    "\n",
    "        # Mask\n",
    "        mask = torch.triu(torch.ones(self.hist_len + self.fct_len,\n",
    "                                     self.hist_len + self.fct_len), diagonal=1) == 1\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        conv_enc = self.conv_encoder(x)\n",
    "        pos_enc = self.pos_encoder(conv_enc)\n",
    "        local_mask = self.mask[:pos_enc.size(-1), :pos_enc.size(-1)]\n",
    "        \n",
    "        transformed = self.transformer_block(pos_enc.permute(2, 0, 1), src_mask=local_mask)\n",
    "\n",
    "        mu = self.mu(transformed)\n",
    "        sigma = self.sigma(self.sigma_raw(transformed))\n",
    "        return transformed, mu, sigma\n",
    "\n",
    "    def step(self, batch, batch_idx, tag=\"train\"):\n",
    "        combined_input = torch.cat(batch, dim=-1)\n",
    "\n",
    "        # Pushing through the network\n",
    "        out, mu, sigma = self(combined_input[:, :, :-1])\n",
    "        loss = self.loss(mu, sigma, combined_input[:, :, 1:].permute(2, 0, 1))\n",
    "        self.log(f'{tag}_logprob', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, tag=\"val\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def sample(self, x, samples):\n",
    "        # Handle single stream or multiple streams\n",
    "        # Note the differences in indexing compared to DeepAR\n",
    "        x = x.view(-1, 1, self.hist_len)\n",
    "\n",
    "        # Initial pass - `mu(T+1)` and `sigma(T+1)` are ready\n",
    "        out, mu, sigma = self(x)\n",
    "\n",
    "        # Sample from the distribution\n",
    "        gaussian = torch.distributions.normal.Normal(mu[-1, :, -1], sigma[-1, :, -1])\n",
    "        initial_sample = gaussian.sample((samples,))\n",
    "\n",
    "        all_samples = []\n",
    "\n",
    "        # Iterating over samples\n",
    "        for sample in range(samples):\n",
    "            current_sample = initial_sample[sample].view(-1, 1, 1)\n",
    "            step_in = torch.cat([x, current_sample], dim=-1)\n",
    "            \n",
    "            # Iterating over time steps\n",
    "            for step in range(self.fct_len - 1):\n",
    "                step_out, mu, sigma = self(step_in[:, :, step+1:])\n",
    "\n",
    "                # Sampling the next step value\n",
    "                gaussian = torch.distributions.normal.Normal(mu[-1, :, -1], sigma[-1, :, -1])\n",
    "                current_sample = gaussian.sample((1,)).view(-1, 1, 1)\n",
    "\n",
    "                # Input tensor for the next step\n",
    "                step_in = torch.cat([step_in, current_sample], dim=-1)\n",
    "            all_samples.append(step_in[:, :, -self.fct_len:].unsqueeze(-1))\n",
    "        return torch.cat(all_samples, dim=-1)\n",
    "\n",
    "    def loss(self, mu, sigma, y):\n",
    "        # Distribution with generated `mu` and `sigma`\n",
    "        gaussian = torch.distributions.normal.Normal(mu, sigma)\n",
    "\n",
    "        # Log-likelihood\n",
    "        L = gaussian.log_prob(y)\n",
    "\n",
    "        return -torch.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElectricityLoadTransformer()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = torch.Tensor(np.random.randn(2, 1, 168))\n",
    "model_output, mu, sigma = model(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output.shape, mu.shape, sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = model.sample(model_input, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "LIMH, LIML =26e3, 9e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dfs = (2 * dfi - LIML - LIMH) / (LIMH - LIML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds = ElectricityDataModule(dfs, batch_size=32)\n",
    "model = ElectricityLoadTransformer(kernels=64, heads=8, dim_feedforward=512)\n",
    "trainer = pl.Trainer(max_epochs=10, progress_bar_refresh_rate=1)\n",
    "trainer.fit(model, ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.setup(\"test\")\n",
    "dl = ds.test_dataloader()\n",
    "\n",
    "hist, fct = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for stream in range(10):\n",
    "    sampled = model.sample(hist[[stream]], 32)\n",
    "    sampled_mean = sampled.mean(dim=-1)\n",
    "    sampled_qhigh = sampled.quantile(0.75, dim=-1)\n",
    "    sampled_qlow = sampled.quantile(0.25, dim=-1)\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "\n",
    "    plt.plot(hist[stream][0], label=\"historical data\")\n",
    "    plt.plot(np.arange(168, 192, 1), fct[stream].detach().numpy()[0], label=\"actual\")\n",
    "    plt.plot(np.arange(168, 192, 1), sampled_mean.detach().numpy()[0, 0],\n",
    "             color=\"white\",\n",
    "             label=\"forecast mean\")\n",
    "    plt.fill_between(np.arange(168, 192, 1),\n",
    "                     sampled_qlow.detach().numpy()[0, 0],\n",
    "                     sampled_qhigh.detach().numpy()[0, 0],\n",
    "                     label=\"forecast interval\", color=\"forestgreen\", alpha=0.6)\n",
    "\n",
    "    plt.legend(loc=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.conv_encoder(hist[[0]]).detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.pos_encoder(model.conv_encoder(hist[[0]])).detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc = model.pos_encoder(model.conv_encoder(hist[[0]])).permute(2, 0, 1)\n",
    "out, attn = model.transformer_block.self_attn(pos_enc, pos_enc, pos_enc, attn_mask=model.mask[:168, :168])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(np.log(1+attn[0].detach().numpy()), vmax=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
